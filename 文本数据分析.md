---
title: 文本数据分析
date: 2017-11-13 20:24:02
categories: Machine Learning
---
*------从未跟你饮过冰，零度天气看风景。*


#### 典型的文本预处理流程
+ 分词：`nltk.word_tokenize`、`nltk.sent_tokenize`、`jieba.cut`
+ 词形归一化：
 stemming：词干提取，去除ing，ed，只保留单词主干。NLTK有`PoterStermmer`、`SnowballStemmer`、`LancasterStemmer`。
 lemmatization：词形归并，将单词的各种词形归并成一种形式，NLTK有`WordNetLemmatizer`，并且可以指定词性。
+ 去除停用词：可以使用nltk.corpus的`stopwords`，也可以自己下载停用词。

<!-- more -->
``` python
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords

# 原始文本
raw_text = 'Life is like a box of chocolates. You never know what you\'re gonna get.'

# 分词
raw_words = nltk.word_tokenize(raw_text)

# 词形归一化
wordnet_lematizer = WordNetLemmatizer()
words = [wordnet_lematizer.lemmatize(raw_word) for raw_word in raw_words]

# 去除停用词
filtered_words = [word for word in words if word not in stopwords.words('english')]

print('原始文本：', raw_text)
print('预处理结果：', filtered_words)

```

#### TF-IDF
TF：Term Frequency(词频)，某个词在该文件中出现的次数
IDF：Inverse Document Frequency(逆文档频率)，衡量某个词普遍的重要性，也有其他计算形式。
TF-IDF=TF*IDF

##### NLTK中的TF-IDF
``` python
from nltk.text import TextCollection

text1 = 'I like the movie so much '
text2 = 'That is a good movie '
text3 = 'This is a great one '
text4 = 'That is a really bad movie '
text5 = 'This is a terrible movie'

# 构建TextCollection对象
tc = TextCollection([text1, text2, text3, 
                        text4, text5])
new_text = 'That one is a good movie. This is so good!'
word = 'That'
tf_idf_val = tc.tf_idf(word, new_text)
print('{}的TF-IDF值为：{}'.format(word, tf_idf_val))

```

##### sklearn中的TF-IDF

``` python
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer()
feat = vectorizer.fit_transform([text1, text2, text3, text4, text5])
feature_names = vectorizer.get_feature_names()
feat_array = feat.toarray()
print(feature_names)
print(feat_array.shape)
print(vectorizer.transform([new_text]).toarray())

```

#### 主题模型及LDA

``` python
import jieba
import gensim
from gensim import corpora, models

ch_text1 = ' 非常失望，剧本完全敷衍了事，主线剧情没突破大家可以理解，可所有的人物都缺乏动机，正邪之间、妇联内部都没什么火花。团结-分裂-团结的三段式虽然老套但其实也可以利用积攒下来的形象魅力搞出意思，但剧本写得非常肤浅、平面。场面上调度混乱呆板，满屏的铁甲审美疲劳。只有笑点算得上差强人意。'
ch_text2 = ' 2015年度最失望作品。以为面面俱到，实则画蛇添足；以为主题深刻，实则老调重弹；以为推陈出新，实则俗不可耐；以为场面很high，实则high劲不足。气！上一集的趣味全无，这集的笑点明显刻意到心虚。全片没有任何片段给我有紧张激动的时候，太弱了，跟奥创一样。'
ch_text3 = ' 《铁人2》中勾引钢铁侠，《妇联1》中勾引鹰眼，《美队2》中勾引美国队长，在《妇联2》中终于……跟绿巨人表白了，黑寡妇用实际行动告诉了我们什么叫忠贞不二；而且为了治疗不孕不育连作战武器都变成了两支验孕棒(坚决相信快银没有死，后面还得回来)'
ch_text4 = ' 虽然从头打到尾，但是真的很无聊啊。'
ch_text5 = ' 剧情不如第一集好玩了，全靠密集笑点在提神。僧多粥少的直接后果就是每部寡姐都要换着队友谈恋爱，这特么比打斗还辛苦啊，真心求放过～～～（结尾彩蛋还以为是洛基呢，结果我呸！）'

ch_texts = [ch_text1, ch_text2, ch_text3, ch_text4, ch_text5]
doc_set = [list(jieba.cut(ch_text, cut_all=False)) for ch_text in ch_texts]
dictionary = corpora.Dictionary(doc_set)
corpus = [ dictionary.doc2bow(doc) for doc in doc_set[:7] ]
lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=5)
lda_model.show_topics()

```

[gesim LDA模型：https://radimrehurek.com/gensim/models/ldamodel.html](https://radimrehurek.com/gensim/models/ldamodel.html)


